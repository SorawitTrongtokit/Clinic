name: Backup Supabase to Google Drive

on:
  schedule:
    # Run every day at 2:00 AM UTC (9:00 AM Thailand)
    - cron: '0 2 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Rclone
        run: |
          sudo apt-get update
          sudo apt-get install -y rclone

      - name: Configure Rclone for Google Drive
        env:
          RCLONE_CONF_SECRET: ${{ secrets.RCLONE_CONFIG }}
        run: |
          # Define a fixed path for config
          mkdir -p ~/.config/rclone
          CONF_PATH="$HOME/.config/rclone/rclone.conf"
          
          # Debug Secret Availability
          SECRET_LEN=${#RCLONE_CONF_SECRET}
          echo "Debug: RCLONE_CONFIG secret length is $SECRET_LEN characters"
          
          if [ "$SECRET_LEN" -eq 0 ]; then
             echo "‚ùå Error: RCLONE_CONFIG secret is EMPTY! Please check GitHub Settings > Secrets."
             exit 1
          fi
          
          # Try 1: Attempt Base64 decode with ignore garbage (-i)
          echo "$RCLONE_CONF_SECRET" | base64 -di > "$CONF_PATH" 2>/dev/null || true
          
          # Validate
          if [ -n "$(rclone listremotes --config="$CONF_PATH")" ]; then
            echo "‚úÖ Config successfully decoded as Base64"
          else
            echo "‚ö†Ô∏è Base64 decode failed or empty. Retrying as Plaintext..."
            echo "$RCLONE_CONF_SECRET" > "$CONF_PATH"
            
            if [ -n "$(rclone listremotes --config="$CONF_PATH")" ]; then
               echo "‚úÖ Config successfully loaded as Plaintext"
            else
               echo "‚ùå Error: Invalid Rclone config."
               echo "Debug: File content (hyphenated):"
               cat "$CONF_PATH" | sed 's/./&-/g' # Obfuscate slightly but check for content
               exit 1
            fi
          fi

      - name: Create backup using Supabase API
        env:
          SUPABASE_PROJECT_REF: ${{ secrets.SUPABASE_PROJECT_REF }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          # ... (rest of the script matches below)

          
          mkdir -p backups
          # List of tables to backup
          TABLES=("patients" "medicines" "visits" "expenses")
          
          for TABLE in "${TABLES[@]}"; do
            echo "Exporting $TABLE..."
            # Note: We limit to 50000 rows per table just in case, but usually API handles streaming okayish for small DBs.
            # Ideally we should implement pagination if DB grows huge.
            curl -s "https://${SUPABASE_PROJECT_REF}.supabase.co/rest/v1/${TABLE}?select=*" \
              -H "apikey: ${SUPABASE_SERVICE_KEY}" \
              -H "Authorization: Bearer ${SUPABASE_SERVICE_KEY}" \
              > "backups/${TABLE}_${TIMESTAMP}.json"
            
            # Check if file is empty or contains error
            if grep -q "error" "backups/${TABLE}_${TIMESTAMP}.json"; then
                echo "Error exporting $TABLE"
                cat "backups/${TABLE}_${TIMESTAMP}.json"
                exit 1
            fi
          done
          
          tar -czvf "backups/clinic_backup_${TIMESTAMP}.tar.gz" backups/*.json
          rm backups/*.json
          
          echo "BACKUP_FILE=backups/clinic_backup_${TIMESTAMP}.tar.gz" >> $GITHUB_ENV
          echo "‚úÖ Backup created: clinic_backup_${TIMESTAMP}.tar.gz"

      - name: Upload to Google Drive
        run: |
          # Detect the remote name dynamically (e.g., could be 'gdrive:', 'remote:', etc.)
          REMOTE_NAME=$(rclone listremotes | head -n 1)
          if [ -z "$REMOTE_NAME" ]; then
            echo "Error: No remote found in rclone config"
            exit 1
          fi
          echo "Using Rclone remote: $REMOTE_NAME"
          
          rclone copy "${{ env.BACKUP_FILE }}" "${REMOTE_NAME}ClinicBackups/" -v
          echo "‚úÖ Uploaded to Google Drive"

      - name: Cleanup old backups on Drive (keep last 30)
        run: |
           # Detect the remote name dynamically again
           REMOTE_NAME=$(rclone listremotes | head -n 1)
           
           # Check if directory exists first
           if rclone lsd "${REMOTE_NAME}ClinicBackups/" >/dev/null 2>&1; then
             rclone lsf "${REMOTE_NAME}ClinicBackups/" --files-only | sort -r | tail -n +31 | while read file; do
               rclone delete "${REMOTE_NAME}ClinicBackups/$file"
               echo "üóëÔ∏è Deleted old backup: $file"
             done
           else
             echo "Info: ClinicBackups directory does not exist yet (first run?), skipping cleanup."
           fi

      - name: Report status
        if: always()
        run: |
          if [ "${{ job.status }}" == "success" ]; then
            echo "‚úÖ Backup completed successfully"
          else
            echo "‚ùå Backup failed"
          fi
